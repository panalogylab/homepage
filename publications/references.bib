
@article{manning_research_nodate,
	title = {A {Research} {Agenda} for {Assessing} the {Economic} {Impacts} of {Code} {Generation} {Models}},
	language = {en},
	author = {Manning, Sam and Mishkin, Pamela and Hadfield, Gillian and Eisner, Emily},
	file = {Manning et al. - A Research Agenda for Assessing the Economic Impac.pdf:/Users/working/Zotero/storage/TF8Y2GW8/Manning et al. - A Research Agenda for Assessing the Economic Impac.pdf:application/pdf},
}

@misc{eloundou_gpts_2023,
	title = {{GPTs} are {GPTs}: {An} {Early} {Look} at the {Labor} {Market} {Impact} {Potential} of {Large} {Language} {Models}},
	shorttitle = {{GPTs} are {GPTs}},
	url = {http://arxiv.org/abs/2303.10130},
	doi = {10.48550/arXiv.2303.10130},
	abstract = {We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of LLMs, while approximately 19\% of workers may see at least 50\% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15\% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56\% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10130 [cs, econ, q-fin]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Economics - General Economics},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/PTPHGEHL/Eloundou et al. - 2023 - GPTs are GPTs An Early Look at the Labor Market I.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/M3N8ZMG9/2303.html:text/html},
}

@article{archibald_probabilistic_2022,
	title = {Probabilistic {Bigraphs}},
	volume = {34},
	issn = {0934-5043},
	url = {https://doi.org/10.1145/3545180},
	doi = {10.1145/3545180},
	abstract = {Bigraphs are a universal computational modelling formalism for the spatial and temporal evolution of a system in which entities can be added and removed. We extend bigraphs to probabilistic bigraphs, and then again to action bigraphs, which include non-determinism and rewards. The extensions are implemented in the BigraphER toolkit and illustrated through examples of virus spread in computer networks and data harvesting in wireless sensor systems. BigraphER also supports the existing stochastic bigraphs extension of Krivine et al. and using BigraphER we give, for the first time, a direct implementation of the membrane budding model used to motivate stochastic bigraphs.},
	number = {2},
	urldate = {2023-02-09},
	journal = {Formal Aspects of Computing},
	author = {Archibald, Blair and Calder, Muffy and Sevegnani, Michele},
	month = sep,
	year = {2022},
	keywords = {Bigraphs, graph rewriting, probabilistic modelling, systems modelling},
	pages = {10:1--10:27},
	file = {Full Text PDF:/Users/working/Zotero/storage/69NMPDGA/Archibald et al. - 2022 - Probabilistic Bigraphs.pdf:application/pdf},
}

@article{conroy-beam_couple_2021,
	title = {Couple {Simulation}: {A} {Novel} {Approach} for {Evaluating} {Models} of {Human} {Mate} {Choice}},
	volume = {25},
	issn = {1088-8683},
	shorttitle = {Couple {Simulation}},
	url = {https://doi.org/10.1177/1088868320971258},
	doi = {10.1177/1088868320971258},
	abstract = {Choosing a mate is perhaps the most important decision a sexually reproducing organism makes in its lifetime. And yet, psychologists lack a precise description of human mate choice, despite sustained attention from several theoretical perspectives. Here, I argue this limited progress owes to the complexity of mate choice and describe a new modeling approach, called ?couple simulation,? designed to compare models of mate choice by challenging them to reproduce real couples within simulated mating markets. I present proof-of-concept simulations that demonstrate couple simulation can identify a population?s true model of mate choice. Furthermore, I apply couple simulation to two samples of real couples and find that the method (a) successfully reconstructs real-world couples, (b) discriminates between models of mate choice, and (c) predicts a wide range of dimensions of relationship quality. Collectively, these results provide evidence that couple simulation offers a framework useful for evaluating theories of human mate choice.},
	language = {en},
	number = {3},
	urldate = {2023-02-26},
	journal = {Personality and Social Psychology Review},
	author = {Conroy-Beam, Daniel},
	month = aug,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {191--228},
}

@misc{wu_recursively_2021,
	title = {Recursively {Summarizing} {Books} with {Human} {Feedback}},
	url = {http://arxiv.org/abs/2109.10862},
	doi = {10.48550/arXiv.2109.10862},
	abstract = {A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases (\${\textbackslash}sim5{\textbackslash}\%\$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
	month = sep,
	year = {2021},
	note = {arXiv:2109.10862 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/CU5UMPK4/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/64BUZ8YJ/2109.html:text/html},
}

@article{bickley_artificial_2022,
	title = {Artificial intelligence in the field of economics},
	volume = {127},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-022-04294-w},
	doi = {10.1007/s11192-022-04294-w},
	abstract = {The history of AI in economics is long and winding, much the same as the evolving field of AI itself. Economists have engaged with AI since its beginnings, albeit in varying degrees and with changing focus across time and places. In this study, we have explored the diffusion of AI and different AI methods (e.g., machine learning, deep learning, neural networks, expert systems, knowledge-based systems) through and within economic subfields, taking a scientometrics approach. In particular, we centre our accompanying discussion of AI in economics around the problems of economic calculation and social planning as proposed by Hayek. To map the history of AI within and between economic sub-fields, we construct two datasets containing bibliometrics information of economics papers based on search query results from the Scopus database and the EconPapers (and IDEAs/RePEc) repository. We present descriptive results that map the use and discussion of AI in economics over time, place, and subfield. In doing so, we also characterise the authors and affiliations of those engaging with AI in economics. Additionally, we find positive correlations between quality of institutional affiliation and engagement with or focus on AI in economics and negative correlations between the Human Development Index and share of learning-based AI papers.},
	language = {en},
	number = {4},
	urldate = {2022-10-10},
	journal = {Scientometrics},
	author = {Bickley, Steve J. and Chan, Ho Fai and Torgler, Benno},
	month = apr,
	year = {2022},
	keywords = {Artificial intelligence, A14, B40, Bibliometrics, Economics, Machine learning, N01, Science of science, Scientometrics},
	pages = {2055--2084},
	file = {Full Text PDF:/Users/working/Zotero/storage/LJ89FHKJ/Bickley et al. - 2022 - Artificial intelligence in the field of economics.pdf:application/pdf},
}

@misc{muennighoff_mteb_2022,
	title = {{MTEB}: {Massive} {Text} {Embedding} {Benchmark}},
	shorttitle = {{MTEB}},
	url = {http://arxiv.org/abs/2210.07316},
	doi = {10.48550/arXiv.2210.07316},
	abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 56 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://huggingface.co/spaces/mteb/leaderboard.},
	urldate = {2022-11-20},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07316 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	annote = {Comment: 23 pages, 14 tables, 6 figures},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/S6M3BVYL/Muennighoff et al. - 2022 - MTEB Massive Text Embedding Benchmark.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/EVCK5MM7/2210.html:text/html},
}

@article{yin_coevolution_2021,
	title = {Coevolution of policy and science during the pandemic},
	volume = {371},
	url = {https://www.science.org/doi/full/10.1126/science.abe3084},
	doi = {10.1126/science.abe3084},
	number = {6525},
	urldate = {2023-02-22},
	journal = {Science},
	author = {Yin, Yian and Gao, Jian and Jones, Benjamin F. and Wang, Dashun},
	month = jan,
	year = {2021},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {128--130},
	file = {Full Text PDF:/Users/working/Zotero/storage/W67GAC24/Yin et al. - 2021 - Coevolution of policy and science during the pande.pdf:application/pdf},
}

@article{silver_reward_2021,
	title = {Reward is enough},
	volume = {299},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370221000862},
	doi = {10.1016/j.artint.2021.103535},
	abstract = {In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.},
	language = {en},
	urldate = {2022-10-08},
	journal = {Artificial Intelligence},
	author = {Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
	month = oct,
	year = {2021},
	keywords = {Artificial general intelligence, Artificial intelligence, Reinforcement learning, Reward},
	pages = {103535},
	file = {ScienceDirect Full Text PDF:/Users/working/Zotero/storage/LJ2CIB5L/Silver et al. - 2021 - Reward is enough.pdf:application/pdf;ScienceDirect Snapshot:/Users/working/Zotero/storage/K3DM2YN2/S0004370221000862.html:text/html},
}

@misc{mendonca_alan_2023,
	title = {{ALAN}: {Autonomously} {Exploring} {Robotic} {Agents} in the {Real} {World}},
	shorttitle = {{ALAN}},
	url = {http://arxiv.org/abs/2302.06604},
	abstract = {Robotic agents that operate autonomously in the real world need to continuously explore their environment and learn from the data collected, with minimal human supervision. While it is possible to build agents that can learn in such a manner without supervision, current methods struggle to scale to the real world. Thus, we propose ALAN, an autonomously exploring robotic agent, that can perform tasks in the real world with little training and interaction time. This is enabled by measuring environment change, which reflects object movement and ignores changes in the robot position. We use this metric directly as an environment-centric signal, and also maximize the uncertainty of predicted environment change, which provides agent-centric exploration signal. We evaluate our approach on two different real-world play kitchen settings, enabling a robot to efficiently explore and discover manipulation skills, and perform tasks specified via goal images. Website at https://robo-explorer.github.io/},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Mendonca, Russell and Bahl, Shikhar and Pathak, Deepak},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06604 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: ICRA 2023. Website at https://robo-explorer.github.io/},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/KB2LE32N/Mendonca et al. - 2023 - ALAN Autonomously Exploring Robotic Agents in the.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/SRX34HLY/2302.html:text/html},
}

@article{bickley_cognitive_2022,
	title = {Cognitive architectures for artificial intelligence ethics},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-022-01452-9},
	doi = {10.1007/s00146-022-01452-9},
	abstract = {As artificial intelligence (AI) thrives and propagates through modern life, a key question to ask is how to include humans in future AI? Despite human involvement at every stage of the production process from conception and design through to implementation, modern AI is still often criticized for its “black box” characteristics. Sometimes, we do not know what really goes on inside or how and why certain conclusions are met. Future AI will face many dilemmas and ethical issues unforeseen by their creators beyond those commonly discussed (e.g., trolley problems and variants of it) and to which solutions cannot be hard-coded and are often still up for debate. Given the sensitivity of such social and ethical dilemmas and the implications of these for human society at large, when and if our AI make the “wrong” choice we need to understand how they got there in order to make corrections and prevent recurrences. This is particularly true in situations where human livelihoods are at stake (e.g., health, well-being, finance, law) or when major individual or household decisions are taken. Doing so requires opening up the “black box” of AI; especially as they act, interact, and adapt in a human world and how they interact with other AI in this world. In this article, we argue for the application of cognitive architectures for ethical AI. In particular, for their potential contributions to AI transparency, explainability, and accountability. We need to understand how our AI get to the solutions they do, and we should seek to do this on a deeper level in terms of the machine-equivalents of motivations, attitudes, values, and so on. The path to future AI is long and winding but it could arrive faster than we think. In order to harness the positive potential outcomes of AI for humans and society (and avoid the negatives), we need to understand AI more fully in the first place and we expect this will simultaneously contribute towards greater understanding of their human counterparts also.},
	language = {en},
	urldate = {2022-10-10},
	journal = {AI \& SOCIETY},
	author = {Bickley, Steve J. and Torgler, Benno},
	month = jun,
	year = {2022},
	keywords = {Artificial intelligence, Cognitive architectures, Ethical AI, Ethics, Intelligent systems, Society},
	file = {Full Text PDF:/Users/working/Zotero/storage/J23VVJKH/Bickley and Torgler - 2022 - Cognitive architectures for artificial intelligenc.pdf:application/pdf},
}

@misc{bai_constitutional_2022,
	title = {Constitutional {AI}: {Harmlessness} from {AI} {Feedback}},
	shorttitle = {Constitutional {AI}},
	url = {http://arxiv.org/abs/2212.08073},
	doi = {10.48550/arXiv.2212.08073},
	abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08073 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/B3RLDW7A/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/IJW96H6G/2212.html:text/html},
}

@inproceedings{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
	urldate = {2023-05-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	year = {2017},
	file = {Full Text PDF:/Users/working/Zotero/storage/3IYYSW9G/Christiano et al. - 2017 - Deep Reinforcement Learning from Human Preferences.pdf:application/pdf},
}

@phdthesis{kwiatkowski_deep_2022,
	type = {{PhD} {Thesis}},
	title = {Deep {Self}-{Modeling} for {Robotic} {Systems}},
	school = {Columbia University},
	author = {Kwiatkowski, Robert},
	year = {2022},
	file = {Full Text:/Users/working/Zotero/storage/GNGSTHDC/Kwiatkowski - 2022 - Deep Self-Modeling for Robotic Systems.pdf:application/pdf;Kwiatkowski_columbia_0054D_17322.pdf:/Users/working/Zotero/storage/ZBTKI742/Kwiatkowski_columbia_0054D_17322.pdf:application/pdf;Snapshot:/Users/working/Zotero/storage/JS8ZLXZ4/1.html:text/html},
}

@misc{naik_discounted_2019,
	title = {Discounted {Reinforcement} {Learning} {Is} {Not} an {Optimization} {Problem}},
	url = {http://arxiv.org/abs/1910.02140},
	doi = {10.48550/arXiv.1910.02140},
	abstract = {Discounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Yao, Hengshuai and Sutton, Richard S.},
	month = nov,
	year = {2019},
	note = {arXiv:1910.02140 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted for presentation at the Optimization Foundations of Reinforcement Learning Workshop at NeurIPS 2019},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/LCANWRS4/Naik et al. - 2019 - Discounted Reinforcement Learning Is Not an Optimi.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/M5RWDBPC/1910.html:text/html},
}

@misc{jiang_general_2022,
	title = {General {Intelligence} {Requires} {Rethinking} {Exploration}},
	url = {http://arxiv.org/abs/2211.07819},
	abstract = {We are at the cusp of a transition from "learning from data" to "learning what data to learn from" as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train our models to how to effectively acquire and use task-relevant data. This problem, which we frame as exploration, is a universal aspect of learning in open-ended domains, such as the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration serves as a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence.},
	urldate = {2022-11-20},
	publisher = {arXiv},
	author = {Jiang, Minqi and Rocktäschel, Tim and Grefenstette, Edward},
	month = nov,
	year = {2022},
	note = {arXiv:2211.07819 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/EDGLE8JN/Jiang et al. - 2022 - General Intelligence Requires Rethinking Explorati.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/B3F6W9JI/2211.html:text/html},
}

@inproceedings{li_internet_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Internet {Curiosity}: {Directed} {Unsupervised} {Learning} on {Uncurated} {Internet} {Data}},
	isbn = {978-3-031-25069-9},
	shorttitle = {Internet {Curiosity}},
	doi = {10.1007/978-3-031-25069-9_7},
	abstract = {We show that a curiosity-driven computer vision algorithm can learn to efficiently query Internet text-to-image search engines for images that improve the model’s performance on a specified dataset. In contrast to typical self-supervised computer vision algorithms, which learn from static datasets, our model actively expands its training set with the most relevant images. First, we calculate an image-level curiosity reward that encourages our model to find the most useful images for pre-training. Second, we use text similarity scores to propagate observed curiosity rewards to untried text queries. This efficiently identifies relevant semantic clusters without any need for class labels or label names from the targeted dataset. Our method significantly outperforms models that require 1–2 orders of magnitude more compute and data.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2022 {Workshops}},
	publisher = {Springer Nature Switzerland},
	author = {Li, Alexander C. and Brown, Ellis and Efros, Alexei A. and Pathak, Deepak},
	editor = {Karlinsky, Leonid and Michaeli, Tomer and Nishino, Ko},
	year = {2023},
	keywords = {Self-supervised learning, Active learning, Curiosity},
	pages = {100--104},
}

@misc{samani_learning_2021,
	title = {Learning {Agent} {State} {Online} with {Recurrent} {Generate}-and-{Test}},
	url = {http://arxiv.org/abs/2112.15236},
	doi = {10.48550/arXiv.2112.15236},
	abstract = {Learning continually and online from a continuous stream of data is challenging, especially for a reinforcement learning agent with sequential data. When the environment only provides observations giving partial information about the state of the environment, the agent must learn the agent state based on the data stream of experience. We refer to the state learned directly from the data stream of experience as the agent state. Recurrent neural networks can learn the agent state, but the training methods are computationally expensive and sensitive to the hyper-parameters, making them unideal for online learning. This work introduces methods based on the generate-and-test approach to learn the agent state. A generate-and-test algorithm searches for state features by generating features and testing their usefulness. In this process, features useful for the agent's performance on the task are preserved, and the least useful features get replaced with newly generated features. We study the effectiveness of our methods on two online multi-step prediction problems. The first problem, trace conditioning, focuses on the agent's ability to remember a cue for a prediction multiple steps into the future. In the second problem, trace patterning, the agent needs to learn patterns in the observation signals and remember them for future predictions. We show that our proposed methods can effectively learn the agent state online and produce accurate predictions.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Samani, Amir and Sutton, Richard S.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.15236 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/FMU75CZF/Samani and Sutton - 2021 - Learning Agent State Online with Recurrent Generat.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/CB9ST5KC/2112.html:text/html},
}

@misc{hong_learning_2023,
	title = {Learning to {Influence} {Human} {Behavior} with {Offline} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2303.02265},
	abstract = {In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies not appearing in the dataset, by utilizing components of diverse, suboptimal interactions. In addition, we demonstrate that offline RL can learn influence that adapts with humans, thus achieving long-term coordination with them even when their behavior changes. We evaluate our proposed method with real people in the Overcooked collaborative benchmark domain, and demonstrate successful improvement in human performance.},
	urldate = {2023-03-09},
	publisher = {arXiv},
	author = {Hong, Joey and Dragan, Anca and Levine, Sergey},
	month = mar,
	year = {2023},
	note = {arXiv:2303.02265 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 12 pages, 7 figures},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/5D5ECZ6N/Hong et al. - 2023 - Learning to Influence Human Behavior with Offline .pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/JJEEMBDR/2303.html:text/html},
}

@inproceedings{bougie_local_2022,
	title = {Local {Control} is {All} {You} {Need}: {Decentralizing} and {Coordinating} {Reinforcement} {Learning} for {Large}-{Scale} {Process} {Control}},
	shorttitle = {Local {Control} is {All} {You} {Need}},
	doi = {10.23919/SICE56594.2022.9905798},
	abstract = {Deep reinforcement learning (RL) approaches are an appealing alternative to conventional controllers in process industries as such methods are inherently flexible and have generalization abilities to unseen situations. Namely, they alleviate the need for constant parameter tuning, tedious design of control laws, and re-identification procedures in the event of performance degradation. However, it remains challenging to apply RL to real-world process tasks, which commonly feature large state-action spaces and complex dynamics. Such tasks may be difficult to solve due to computational complexity and sample insufficiency. To tackle these limitations, we present a sample-efficient RL approach for large-scale control that expresses the global policy as a collection of local policies. Every local policy receives local observations and is responsible for controlling a different region of the environment. In order to enable coordination among local policies, we present a mechanism based on action sharing and message passing. The model is evaluated on a set of robotic tasks and a large-scale vinyl acetate monomer (VAM) plant. The experiments demonstrate that the proposed model exhibits significant improvements over baselines in terms of mean scores and sample efficiency.},
	booktitle = {2022 61st {Annual} {Conference} of the {Society} of {Instrument} and {Control} {Engineers} ({SICE})},
	author = {Bougie, Nicolas and Onishi, Takashi and Tsuruoka, Yoshimasa},
	month = sep,
	year = {2022},
	keywords = {Reinforcement learning, Aerospace electronics, chemical process control, Communication channels, deep reinforcement learning, large-scale reinforcement learning, Message passing, Process control, Robot kinematics, Training},
	pages = {468--474},
	file = {IEEE Xplore Abstract Record:/Users/working/Zotero/storage/IYTYZXAD/9905798.html:text/html},
}

@misc{kwiatkowski_origins_2022,
	title = {On the {Origins} of {Self}-{Modeling}},
	url = {http://arxiv.org/abs/2209.02010},
	doi = {10.48550/arXiv.2209.02010},
	abstract = {Self-Modeling is the process by which an agent, such as an animal or machine, learns to create a predictive model of its own dynamics. Once captured, this self-model can then allow the agent to plan and evaluate various potential behaviors internally using the self-model, rather than using costly physical experimentation. Here, we quantify the benefits of such self-modeling against the complexity of the robot. We find a R2 =0.90 correlation between the number of degrees of freedom a robot has, and the added value of self-modeling as compared to a direct learning baseline. This result may help motivate self modeling in increasingly complex robotic systems, as well as shed light on the origins of self-modeling, and ultimately self-awareness, in animals and humans.},
	urldate = {2022-10-18},
	publisher = {arXiv},
	author = {Kwiatkowski, Robert and Hu, Yuhang and Chen, Boyuan and Lipson, Hod},
	month = sep,
	year = {2022},
	note = {arXiv:2209.02010 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1910.01994},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/USEKRGT9/Kwiatkowski et al. - 2022 - On the Origins of Self-Modeling.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/9PNZQ4JN/2209.html:text/html},
}

@misc{anthony_policy_2019,
	title = {Policy {Gradient} {Search}: {Online} {Planning} and {Expert} {Iteration} without {Search} {Trees}},
	shorttitle = {Policy {Gradient} {Search}},
	url = {http://arxiv.org/abs/1904.03646},
	abstract = {Monte Carlo Tree Search (MCTS) algorithms perform simulation-based search to improve policies online. During search, the simulation policy is adapted to explore the most promising lines of play. MCTS has been used by state-of-the-art programs for many problems, however a disadvantage to MCTS is that it estimates the values of states with Monte Carlo averages, stored in a search tree; this does not scale to games with very high branching factors. We propose an alternative simulation-based search method, Policy Gradient Search (PGS), which adapts a neural network simulation policy online via policy gradient updates, avoiding the need for a search tree. In Hex, PGS achieves comparable performance to MCTS, and an agent trained using Expert Iteration with PGS was able defeat MoHex 2.0, the strongest open-source Hex agent, in 9x9 Hex.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Anthony, Thomas and Nishihara, Robert and Moritz, Philipp and Salimans, Tim and Schulman, John},
	month = apr,
	year = {2019},
	note = {arXiv:1904.03646 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/DNTYDE44/Anthony et al. - 2019 - Policy Gradient Search Online Planning and Expert.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/K86IAMGS/1904.html:text/html},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/JGTTDAR6/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/3ZJGPQXX/1707.html:text/html},
}

@article{javed_scalable_nodate,
	title = {Scalable {Online} {State} {Construction} using {Recurrent} {Networks}},
	abstract = {State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Two popular gradient-based methods for recurrent learning are back-propagation through time (BPTT), and real-time recurrent learning (RTRL). BPTT looks at the complete sequence of observations before computing gradients and is unsuitable for online updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules or learning a recurrent network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add bias or noise to the gradient estimate. Instead, they trade off the functional capacity of the recurrent network to achieve scalable learning. We demonstrate the effectiveness of our approach on a prediction learning benchmark inspired by animal learning.},
	language = {en},
	author = {Javed, Khurram and Shah, Haseeb},
	pages = {5},
	file = {Javed and Shah - Scalable Online State Construction using Recurrent.pdf:/Users/working/Zotero/storage/5YBFDLVI/Javed and Shah - Scalable Online State Construction using Recurrent.pdf:application/pdf},
}

@techreport{hu_self-supervised_2023,
	title = {Self-supervised robot self-modeling using a single egocentric camer},
	copyright = {cc by},
	url = {https://doi.org/10.21203/rs.3.rs-2303274/v1},
	abstract = {The ability of robots to model their own dynamics is key to autonomous planning and learning, as well as for autonomous damage detection and recovery. Traditionally dynamic models are pre-programmed, or learned from external observations and IMU data. Here, we demonstrate for the first time how a task-agnostic dynamic self-model can be learned using only a single first-person-view camera in a self-supervised manner, without any prior knowledge of robot morphology, kinematics, or task. We trained an egocentric visual self-model using random motor babbling on a 12-DoF robot. We then show how the robot can leverage its visual self-model to achieve various locomotion tasks, such as moving forward, backward and turning, all without any additional physical training. The accuracy of the egocentric model exceeds that of a model trained using an IMU. We also show how a robot can automatically detect and recover from damage. We suggest that self-supervised egocentric visual self-modeling could allow complex systems to continuously model themselves without additional sensors and prior knowledge.},
	urldate = {2023-02-18},
	author = {Hu, Yuhang and Chen, Boyuan and Lipson, Hod},
	year = {2023},
	doi = {10.21203/rs.3.rs-2303274/v1},
	note = {Type: article},
	file = {Submitted Version:/Users/working/Zotero/storage/V5FPXUDY/Hu et al. - 2023 - Self-supervised robot self-modeling using a single.pdf:application/pdf},
}

@misc{sutton_alberta_2022,
	title = {The {Alberta} {Plan} for {AI} {Research}},
	url = {http://arxiv.org/abs/2208.11173},
	abstract = {Herein we describe our approach to artificial intelligence research, which we call the Alberta Plan. The Alberta Plan is pursued within our research groups in Alberta and by others who are like minded throughout the world. We welcome all who would join us in this pursuit.},
	urldate = {2023-02-09},
	publisher = {arXiv},
	author = {Sutton, Richard S. and Bowling, Michael and Pilarski, Patrick M.},
	month = oct,
	year = {2022},
	note = {arXiv:2208.11173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/CMDUGUFH/Sutton et al. - 2022 - The Alberta Plan for AI Research.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/WRB6C6LB/2208.html:text/html},
}

@misc{sutton_quest_2022,
	title = {The {Quest} for a {Common} {Model} of the {Intelligent} {Decision} {Maker}},
	url = {http://arxiv.org/abs/2202.13252},
	doi = {10.48550/arXiv.2202.13252},
	abstract = {The premise of the Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Sutton, Richard S.},
	month = jun,
	year = {2022},
	note = {arXiv:2202.13252 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Will appear as an extended abstract at the fifth Multi-disciplinary Conference on Reinforcement Learning and Decision Making, held in Providence, Rhode Island, June 8-11, 2022},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/WN3CE2ME/Sutton - 2022 - The Quest for a Common Model of the Intelligent De.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/FCWL9VJR/2202.html:text/html},
}

@misc{zhang_wisdom_2023,
	title = {The {Wisdom} of {Hindsight} {Makes} {Language} {Models} {Better} {Instruction} {Followers}},
	url = {http://arxiv.org/abs/2302.05206},
	doi = {10.48550/arXiv.2302.05206},
	abstract = {Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Zhang, Tianjun and Liu, Fangchen and Wong, Justin and Abbeel, Pieter and Gonzalez, Joseph E.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.05206 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/K5YVZS2R/Zhang et al. - 2023 - The Wisdom of Hindsight Makes Language Models Bett.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/386TEAKE/2302.html:text/html},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/SAQ9QJS8/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/IFQL3BZA/2203.html:text/html},
}

@inproceedings{mahmoudieh_zero-shot_2022,
	title = {Zero-{Shot} {Reward} {Specification} via {Grounded} {Natural} {Language}},
	url = {https://openreview.net/forum?id=BxZxTN6Ek-9},
	abstract = {Reward signals in reinforcement learning are expensive to design and often require access to the true state which is not available in the real world. Common alternatives are usually demonstrations...},
	language = {en},
	urldate = {2022-10-09},
	author = {Mahmoudieh, Parsa and Pathak, Deepak and Darrell, Trevor},
	month = apr,
	year = {2022},
	file = {Full Text PDF:/Users/working/Zotero/storage/I9NYW3JE/Mahmoudieh et al. - 2022 - Zero-Shot Reward Specification via Grounded Natura.pdf:application/pdf},
}

@article{drori_neural_2022,
	title = {A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level},
	volume = {119},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2123433119},
	doi = {10.1073/pnas.2123433119},
	abstract = {We demonstrate that a neural network automatically solves, explains, and generates university-level problems from the largest Massachusetts Institute of Technology (MIT) mathematics courses at a human level. Our methods combine three innovations: 1) using recent neural networks pretrained on text and fine-tuned on code rather than pretrained on text; 2) few-shot learning synthesizing programs that correctly solve course problems automatically; and 3) a pipeline to solve questions, explain solutions, and generate new questions indistinguishable by students from course questions. Our work solves university-level mathematics courses and improves upon state-of-the-art, increasing automatic accuracy on randomly sampled questions on a benchmark by order of magnitude. Implications for higher education include roles of artificial intelligence (AI) in automated course evaluation and content generation.
We demonstrate that a neural network pretrained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI’s Codex transformer and execute them to solve course problems at 81\% automatic accuracy. We curate a dataset of questions from Massachusetts Institute of Technology (MIT)’s largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University’s Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pretrained on text automatically solves only 18.8\% of these university questions using zero-shot learning and 30.8\% using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81\% of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8 to 81.1\%. We perform a survey to evaluate the quality and difficulty of generated questions. This work automatically solves university-level mathematics course questions at a human level and explains and generates university-level mathematics course questions at scale, a milestone for higher education.},
	number = {32},
	urldate = {2022-10-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and Wang, Roman and Singh, Nikhil and Patti, Taylor L. and Lynch, Jayson and Shporer, Avi and Verma, Nakul and Wu, Eugene and Strang, Gilbert},
	month = aug,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2123433119},
	file = {Full Text PDF:/Users/working/Zotero/storage/YV7V44VQ/Drori et al. - 2022 - A neural network solves, explains, and generates u.pdf:application/pdf},
}

@misc{kirchenbauer_watermark_2023,
	title = {A {Watermark} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2301.10226},
	doi = {10.48550/arXiv.2301.10226},
	abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10226 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: 12 pages in the main body. Code will be available at github.com/jwkirchenbauer/lm-watermarking},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/MKVNT5S4/Kirchenbauer et al. - 2023 - A Watermark for Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/2CQ9QQT3/2301.html:text/html},
}

@article{russell_artificial_2021,
	title = {Artificial intelligence: a modern approach, global edition 4th},
	volume = {19},
	shorttitle = {Artificial intelligence},
	journal = {Foundations},
	author = {Russell, Stuart and Norvig, Peter},
	year = {2021},
	pages = {23},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/W94ZYKUG/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/S46P9QX5/1706.html:text/html},
}

@misc{mialon_augmented_2023,
	title = {Augmented {Language} {Models}: a {Survey}},
	shorttitle = {Augmented {Language} {Models}},
	url = {http://arxiv.org/abs/2302.07842},
	doi = {10.48550/arXiv.2302.07842},
	abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Mialon, Grégoire and Dessì, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozière, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07842 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/9LRS4QSI/Mialon et al. - 2023 - Augmented Language Models a Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/3Q2XT7ZK/2302.html:text/html},
}

@misc{srivastava_beyond_2022,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ramírez, César Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and de Melo, Gerard and Kruszewski, Germán and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and Jaimovitch-López, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ramón Risco and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computers and Society, Statistics - Machine Learning},
	annote = {Comment: 27 pages, 17 figures + references and appendices, repo: https://github.com/google/BIG-bench},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/UFAU2AZL/Srivastava et al. - 2022 - Beyond the Imitation Game Quantifying and extrapo.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/9QCTLXAA/2206.html:text/html},
}

@misc{weidinger_ethical_2021,
	title = {Ethical and social risks of harm from {Language} {Models}},
	url = {http://arxiv.org/abs/2112.04359},
	doi = {10.48550/arXiv.2112.04359},
	abstract = {This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and Kenton, Zac and Brown, Sasha and Hawkins, Will and Stepleton, Tom and Biles, Courtney and Birhane, Abeba and Haas, Julia and Rimell, Laura and Hendricks, Lisa Anne and Isaac, William and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = dec,
	year = {2021},
	note = {arXiv:2112.04359 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/4JGPQQB7/Weidinger et al. - 2021 - Ethical and social risks of harm from Language Mod.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/VH3BYEKR/2112.html:text/html},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: corrected typos, added references, added authors, added acknowledgements},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/MBWLX49K/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/FLY9GCJU/2107.html:text/html},
}

@misc{goldstein_generative_2023,
	title = {Generative {Language} {Models} and {Automated} {Influence} {Operations}: {Emerging} {Threats} and {Potential} {Mitigations}},
	shorttitle = {Generative {Language} {Models} and {Automated} {Influence} {Operations}},
	url = {http://arxiv.org/abs/2301.04246},
	doi = {10.48550/arXiv.2301.04246},
	abstract = {Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Goldstein, Josh A. and Sastry, Girish and Musser, Micah and DiResta, Renee and Gentzel, Matthew and Sedova, Katerina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.04246 [cs]},
	keywords = {Computer Science - Computers and Society},
	annote = {Comment: 82 pages, 26 figures},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/H38WRJWD/Goldstein et al. - 2023 - Generative Language Models and Automated Influence.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/HQ5Q97U6/2301.html:text/html},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/RY4BCSH6/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/5BMZV7EV/2204.html:text/html},
}

@misc{mu_improving_2022,
	title = {Improving {Intrinsic} {Exploration} with {Language} {Abstractions}},
	url = {http://arxiv.org/abs/2202.08938},
	doi = {10.48550/arXiv.2202.08938},
	abstract = {Reinforcement learning (RL) agents are particularly hard to train when rewards are sparse. One common solution is to use intrinsic rewards to encourage agents to explore their environment. However, recent intrinsic exploration methods often use state-based novelty measures which reward low-level exploration and may not scale to domains requiring more abstract skills. Instead, we explore natural language as a general medium for highlighting relevant abstractions in an environment. Unlike previous work, we evaluate whether language can improve over existing exploration methods by directly extending (and comparing to) competitive intrinsic exploration baselines: AMIGo (Campero et al., 2021) and NovelD (Zhang et al., 2021). These language-based variants outperform their non-linguistic forms by 45-85\% across 13 challenging tasks from the MiniGrid and MiniHack environment suites.},
	urldate = {2022-11-20},
	publisher = {arXiv},
	author = {Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah and Rocktäschel, Tim and Grefenstette, Edward},
	month = feb,
	year = {2022},
	note = {arXiv:2202.08938 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/7T75TGCY/Mu et al. - 2022 - Improving Intrinsic Exploration with Language Abst.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/56FBLZSD/2202.html:text/html},
}

@misc{yin_interpreting_2022,
	title = {Interpreting {Language} {Models} with {Contrastive} {Explanations}},
	url = {http://arxiv.org/abs/2202.10419},
	doi = {10.48550/arXiv.2202.10419},
	abstract = {Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics. Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding. To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.},
	urldate = {2023-02-02},
	publisher = {arXiv},
	author = {Yin, Kayo and Neubig, Graham},
	month = may,
	year = {2022},
	note = {arXiv:2202.10419 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/YRFXM7QC/Yin and Neubig - 2022 - Interpreting Language Models with Contrastive Expl.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/J99RFG3C/2202.html:text/html},
}

@article{ogueji_intriguing_2022,
	title = {Intriguing {Properties} of {Compression} on {Multilingual} {Models}},
	url = {http://arxiv.org/abs/2211.02738},
	doi = {10.48550/arXiv.2211.02738},
	abstract = {Multilingual models are often particularly dependent on scaling to generalize to a growing number of languages. Compression techniques are widely relied upon to reconcile the growth in model size with real world resource constraints, but compression can have a disparate effect on model performance for low-resource languages. It is thus crucial to understand the trade-offs between scale, multilingualism, and compression. In this work, we propose an experimental framework to characterize the impact of sparsifying multilingual pre-trained language models during fine-tuning. Applying this framework to mBERT named entity recognition models across 40 languages, we find that compression confers several intriguing and previously unknown generalization properties. In contrast to prior findings, we find that compression may improve model robustness over dense models. We additionally observe that under certain sparsification regimes compression may aid, rather than disproportionately impact the performance of low-resource languages.},
	urldate = {2022-12-16},
	author = {Ogueji, Kelechi and Ahia, Orevaoghene and Onilude, Gbemileke and Gehrmann, Sebastian and Hooker, Sara and Kreutzer, Julia},
	year = {2022},
	note = {arXiv:2211.02738 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2022},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/62F24ZZC/Ogueji et al. - 2022 - Intriguing Properties of Compression on Multilingu.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/TUS6MT7A/2211.html:text/html},
}

@misc{thoppilan_lamda_2022,
	title = {{LaMDA}: {Language} {Models} for {Dialog} {Applications}},
	shorttitle = {{LaMDA}},
	url = {http://arxiv.org/abs/2201.08239},
	doi = {10.48550/arXiv.2201.08239},
	abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
	month = feb,
	year = {2022},
	note = {arXiv:2201.08239 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/LECHPZGJ/Thoppilan et al. - 2022 - LaMDA Language Models for Dialog Applications.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/PJNYKQEZ/2201.html:text/html},
}

@misc{huang_language_2022,
	title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
	shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
	url = {http://arxiv.org/abs/2201.07207},
	doi = {10.48550/arXiv.2201.07207},
	abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
	urldate = {2022-10-09},
	publisher = {arXiv},
	author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
	month = mar,
	year = {2022},
	note = {arXiv:2201.07207 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: Project website at https://huangwl18.github.io/language-planner},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/KEIGUUXG/Huang et al. - 2022 - Language Models as Zero-Shot Planners Extracting .pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/GBDDRQVZ/2201.html:text/html},
}

@misc{ho_large_2022,
	title = {Large {Language} {Models} {Are} {Reasoning} {Teachers}},
	url = {http://arxiv.org/abs/2212.10071},
	doi = {10.48550/arXiv.2212.10071},
	abstract = {Language models (LMs) have demonstrated remarkable performance on downstream tasks, using in-context exemplars or human instructions. Recent works have shown that chain-of-thought (CoT) prompting can elicit models to solve complex reasoning tasks, step-by-step. However, the efficacy of prompt-based CoT methods is restricted to very large LMs such as GPT-3 (175B), thus limiting deployability. In this paper, we revisit the fine-tuning approach to enable complex reasoning in smaller LMs, optimized to efficiently perform a specific task. We propose Fine-tune-CoT, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via fine-tuning. We evaluate our method on publicly available LMs across a wide range of complex tasks and model sizes. We find that Fine-tune-CoT enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance. Student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. We conduct extensive ablations and sample studies to understand the reasoning capabilities of student models. We also identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT and address them in our analysis.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10071 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/WEGEZVK4/Ho et al. - 2022 - Large Language Models Are Reasoning Teachers.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/BV9ZUI2H/2212.html:text/html},
}

@misc{kandpal_large_2022,
	title = {Large {Language} {Models} {Struggle} to {Learn} {Long}-{Tail} {Knowledge}},
	url = {http://arxiv.org/abs/2211.08411},
	doi = {10.48550/arXiv.2211.08411},
	abstract = {The internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, there is a huge variability in the number of times a given piece of information appears on the web. In this paper, we study the relationship between the knowledge memorized by large language models and the information in their pre-training datasets. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, we find that while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant document count, presenting a promising approach for capturing the long-tail.},
	urldate = {2022-12-16},
	publisher = {arXiv},
	author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
	month = nov,
	year = {2022},
	note = {arXiv:2211.08411 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/U6LAZ2GL/Kandpal et al. - 2022 - Large Language Models Struggle to Learn Long-Tail .pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/3385HZ6C/2211.html:text/html},
}

@inproceedings{stiennon_learning_2020,
	title = {Learning to summarize with human feedback},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html},
	abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task.  For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences.  We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning.  We conduct extensive analyses to understand our human feedback dataset and fine-tuned models.  We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans.  We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
	urldate = {2023-02-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
	year = {2020},
	pages = {3008--3021},
	file = {Full Text PDF:/Users/working/Zotero/storage/3BDVEH9W/Stiennon et al. - 2020 - Learning to summarize with human feedback.pdf:application/pdf},
}

@article{touvron_llama_nodate,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1.},
	language = {en},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	file = {Touvron et al. - LLaMA Open and Efficient Foundation Language Mode.pdf:/Users/working/Zotero/storage/TFFKQAZH/Touvron et al. - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf},
}

@misc{noauthor_openaievals_nodate,
	title = {openai/evals: {Evals} is a framework for evaluating {LLMs} and {LLM} systems, and an open-source registry of benchmarks.},
	url = {https://github.com/openai/evals},
	urldate = {2023-05-31},
	file = {openai/evals\: Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.:/Users/working/Zotero/storage/96NMAXJP/evals.html:text/html},
}

@misc{schick_peer_2022,
	title = {{PEER}: {A} {Collaborative} {Language} {Model}},
	shorttitle = {{PEER}},
	url = {http://arxiv.org/abs/2208.11663},
	doi = {10.48550/arXiv.2208.11663},
	abstract = {Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Jiang, Zhengbao and Petroni, Fabio and Lewis, Patrick and Izacard, Gautier and You, Qingfei and Nalmpantis, Christoforos and Grave, Edouard and Riedel, Sebastian},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11663 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/XNJ5E6ZV/Schick et al. - 2022 - PEER A Collaborative Language Model.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/H5DFE6KJ/2208.html:text/html},
}

@misc{chen_program_2022,
	title = {Program of {Thoughts} {Prompting}: {Disentangling} {Computation} from {Reasoning} for {Numerical} {Reasoning} {Tasks}},
	shorttitle = {Program of {Thoughts} {Prompting}},
	url = {http://arxiv.org/abs/2211.12588},
	abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12{\textbackslash}\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github{\textbackslash}footnote\{{\textbackslash}url\{https://github.com/wenhuchen/Program-of-Thoughts\}\}.},
	urldate = {2022-11-27},
	publisher = {arXiv},
	author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
	month = nov,
	year = {2022},
	note = {arXiv:2211.12588 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/X84DGG3X/Chen et al. - 2022 - Program of Thoughts Prompting Disentangling Compu.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/HZ3FEUXB/2211.html:text/html},
}

@misc{whittington_relating_2022,
	title = {Relating transformers to models and neural representations of the hippocampal formation},
	url = {http://arxiv.org/abs/2112.04035},
	abstract = {Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Whittington, James C. R. and Warren, Joseph and Behrens, Timothy E. J.},
	month = mar,
	year = {2022},
	note = {arXiv:2112.04035 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/D5BNIT6Z/Whittington et al. - 2022 - Relating transformers to models and neural represe.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/PNV9WAAJ/2112.html:text/html},
}

@misc{gudibande_false_2023,
	title = {The {False} {Promise} of {Imitating} {Proprietary} {LLMs}},
	url = {http://arxiv.org/abs/2305.15717},
	doi = {10.48550/arXiv.2305.15717},
	abstract = {An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
	month = may,
	year = {2023},
	note = {arXiv:2305.15717 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/887446J2/Gudibande et al. - 2023 - The False Promise of Imitating Proprietary LLMs.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/ERP3W4XN/2305.html:text/html},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/IWF9UCUT/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/ZLPPBBXK/2203.html:text/html},
}

@misc{wen_transformers_2022,
	title = {Transformers in {Time} {Series}: {A} {Survey}},
	shorttitle = {Transformers in {Time} {Series}},
	url = {http://arxiv.org/abs/2202.07125},
	doi = {10.48550/arXiv.2202.07125},
	abstract = {Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also intrigues great interests in the time series community. Among multiple advantages of transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review transformer schemes for time series modeling by highlighting their strengths as well as limitations through a new taxonomy to summarize existing time series transformers in two perspectives. From the perspective of network modifications, we summarize the adaptations of module level and architecture level of the time series transformers. From the perspective of applications, we categorize time series transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. A corresponding resource list that will be continuously updated can be found in the GitHub repository. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.},
	urldate = {2023-02-07},
	publisher = {arXiv},
	author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
	month = mar,
	year = {2022},
	note = {arXiv:2202.07125 [cs, eess, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	annote = {Comment: 8 pages, 65 referred papers; The corresponding GitHub repository is https://github.com/qingsongedu/time-series-transformers-review},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/IAM8XMQC/Wen et al. - 2022 - Transformers in Time Series A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/Z5S24EDY/2202.html:text/html},
}

@misc{hendrycks_unsolved_2022,
	title = {Unsolved {Problems} in {ML} {Safety}},
	url = {http://arxiv.org/abs/2109.13916},
	doi = {10.48550/arXiv.2109.13916},
	abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
	urldate = {2023-01-04},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
	month = jun,
	year = {2022},
	note = {arXiv:2109.13916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Position Paper},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/NS3WDT3Y/Hendrycks et al. - 2022 - Unsolved Problems in ML Safety.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/JXFTNXFA/2109.html:text/html},
}

@misc{derakhshani_variational_2022,
	title = {Variational prompt tuning improves generalization of vision-language models},
	url = {http://arxiv.org/abs/2210.02390},
	abstract = {Prompt tuning provides an efficient mechanism to adapt large vision-language models to downstream tasks by treating part of the input language prompts as learnable parameters while freezing the rest of the model. Existing works for prompt tuning are however prone to damaging the generalization capabilities of the foundation models, because the learned prompts lack the capacity of covering certain concepts within the language model. To avoid such limitation, we propose a probabilistic modeling of the underlying distribution of prompts, allowing prompts within the support of an associated concept to be derived through stochastic sampling. This results in a more complete and richer transfer of the information captured by the language model, providing better generalization capabilities for downstream tasks. The resulting algorithm relies on a simple yet powerful variational framework that can be directly integrated with other developments. We show our approach is seamlessly integrated into both standard and conditional prompt learning frameworks, improving the performance on both cases considerably, especially with regards to preserving the generalization capability of the original model. Our method provides the current state-of-the-art for prompt learning, surpassing CoCoOp by 1.6\% average Top-1 accuracy on the standard benchmark. Remarkably, it even surpasses the original CLIP model in terms of generalization to new classes. Implementation code will be released.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Derakhshani, Mohammad Mahdi and Sanchez, Enrique and Bulat, Adrian and da Costa, Victor Guilherme Turrisi and Snoek, Cees G. M. and Tzimiropoulos, Georgios and Martinez, Brais},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02390 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/U5FUKL3A/Derakhshani et al. - 2022 - Variational prompt tuning improves generalization .pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/BB5WJ8D6/2210.html:text/html},
}

@misc{banchio_artificial_2022,
	title = {Artificial {Intelligence} and {Auction} {Design}},
	url = {http://arxiv.org/abs/2202.05947},
	doi = {10.48550/arXiv.2202.05947},
	abstract = {Motivated by online advertising auctions, we study auction design in repeated auctions played by simple Artificial Intelligence algorithms (Q-learning). We find that first-price auctions with no additional feedback lead to tacit-collusive outcomes (bids lower than values), while second-price auctions do not. We show that the difference is driven by the incentive in first-price auctions to outbid opponents by just one bid increment. This facilitates re-coordination on low bids after a phase of experimentation. We also show that providing information about lowest bid to win, as introduced by Google at the time of switch to first-price auctions, increases competitiveness of auctions.},
	urldate = {2022-11-03},
	publisher = {arXiv},
	author = {Banchio, Martino and Skrzypacz, Andrzej},
	month = feb,
	year = {2022},
	note = {arXiv:2202.05947 [cs, econ]},
	keywords = {Computer Science - Artificial Intelligence, Economics - Theoretical Economics, Computer Science - Computer Science and Game Theory},
	annote = {Comment: 30 pages, 11 figures},
	file = {arXiv Fulltext PDF:/Users/working/Zotero/storage/EM6WLSYX/Banchio and Skrzypacz - 2022 - Artificial Intelligence and Auction Design.pdf:application/pdf;arXiv.org Snapshot:/Users/working/Zotero/storage/VCT69GH6/2202.html:text/html},
}

@article{khezr_review_2022,
	title = {A review of multiunit auctions with homogeneous goods},
	volume = {36},
	issn = {1467-6419},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12482},
	doi = {10.1111/joes.12482},
	abstract = {This article provides a comprehensive survey of the literature on multiunit auctions with multiple homogeneous goods. Despite the importance of multiunit auctions, the research on these auctions attracted substantially less attention than single-unit auctions. We start by providing a review of the theoretical literature on homogeneous good multiunit auctions where bidders have multiunit demands. The articles are categorized based on their models' assumptions regarding bidders' values and the type of auction. Furthermore, we review the empirical and experimental evidence related to these multiunit auctions to understand the relationship with the theoretical findings.},
	language = {en},
	number = {4},
	urldate = {2023-03-15},
	journal = {Journal of Economic Surveys},
	author = {Khezr, Peyman and Cumpston, Anne},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12482},
	keywords = {auctions, discriminatory, multiunit, uniform},
	pages = {1225--1247},
	file = {Snapshot:/Users/working/Zotero/storage/L7S7J567/joes.html:text/html},
}
